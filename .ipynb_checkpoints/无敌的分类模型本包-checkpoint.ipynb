{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046a0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    PassiveAggressiveClassifier,\n",
    "    Perceptron,\n",
    "    RidgeClassifier,\n",
    "    SGDClassifier,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "class ClassificationObjectives:\n",
    "    def __init__(\n",
    "        self, X, y, cv=5, random_state=42, metric=\"accuracy\", lgbm_objective=\"binary\"\n",
    "    ):\n",
    "        self.metric = metric\n",
    "        self.cv = cv\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.random_state = random_state\n",
    "        self.lgbm_objective = lgbm_objective\n",
    "\n",
    "    def lr_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 1e-5, 1e5),\n",
    "            \"solver\": trial.suggest_categorical(\n",
    "                \"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "            ),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 1, 1000),\n",
    "            \"tol\": trial.suggest_loguniform(\"tol\", 1e-5, 1e-2),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = LogisticRegression(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def sgd_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"loss\": trial.suggest_categorical(\n",
    "                \"loss\",\n",
    "                [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "            ),\n",
    "            \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\", \"elasticnet\"]),\n",
    "            \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-5, 1e5),\n",
    "            \"l1_ratio\": trial.suggest_uniform(\"l1_ratio\", 0, 1),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 1, 1000),\n",
    "            \"tol\": trial.suggest_loguniform(\"tol\", 1e-5, 1e-2),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = SGDClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf, self.X, self.y, cv=self.cv, scoring=self.metric, n_jobs=-1\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def ridg_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-5, 1e5),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 1, 1000),\n",
    "            \"tol\": trial.suggest_loguniform(\"tol\", 1e-5, 1e-2),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = RidgeClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def perc_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\", \"elasticnet\"]),\n",
    "            \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-5, 1e5),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 1, 1000),\n",
    "            \"tol\": trial.suggest_loguniform(\"tol\", 1e-5, 1e-2),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = Perceptron(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def pass_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 1e-5, 1e5),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 1, 1000),\n",
    "            \"tol\": trial.suggest_loguniform(\"tol\", 1e-5, 1e-2),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = PassiveAggressiveClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def svm_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 1e-5, 1e5),\n",
    "            \"kernel\": trial.suggest_categorical(\n",
    "                \"kernel\", [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "            ),\n",
    "            \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-5, 1e5),\n",
    "            \"degree\": trial.suggest_int(\"degree\", 1, 10),\n",
    "            \"coef0\": trial.suggest_loguniform(\"coef0\", 1e-5, 1e5),\n",
    "            \"shrinking\": trial.suggest_categorical(\"shrinking\", [True, False]),\n",
    "            \"tol\": trial.suggest_loguniform(\"tol\", 1e-5, 1e-2),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = SVC(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def knn_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 256),\n",
    "            \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\": trial.suggest_int(\"p\", 1, 10),\n",
    "            \"n_jobs\": -1,\n",
    "            \"rows_limit\": 100000,\n",
    "        }\n",
    "        clf = KNeighborsClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def dt_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"splitter\": trial.suggest_categorical(\"splitter\", [\"best\", \"random\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 1, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "            \"min_weight_fraction_leaf\": trial.suggest_float(\n",
    "                \"min_weight_fraction_leaf\", 0, 0.5\n",
    "            ),\n",
    "            \"max_features\": trial.suggest_categorical(\n",
    "                \"max_features\", [\"auto\", \"sqrt\", \"log2\"]\n",
    "            ),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = DecisionTreeClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def rfc_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"min_weight_fraction_leaf\": trial.suggest_float(\n",
    "                \"min_weight_fraction_leaf\", 0, 0.5\n",
    "            ),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 100),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 100),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.01, 1),\n",
    "            \"seed\":  self.random_state,\n",
    "            \"n_jobs\": -1,\n",
    "            \"max_steps\": 10,\n",
    "        }\n",
    "        clf = RandomForestClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def gbc_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e5),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 1, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "            \"min_weight_fraction_leaf\": trial.suggest_float(\n",
    "                \"min_weight_fraction_leaf\", 0, 0.5\n",
    "            ),\n",
    "            \"max_features\": trial.suggest_categorical(\n",
    "                \"max_features\", [\"auto\", \"sqrt\", \"log2\"]\n",
    "            ),\n",
    "        }\n",
    "        clf = GradientBoostingClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def ada_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e5),\n",
    "            \"algorithm\": trial.suggest_categorical(\"algorithm\", [\"SAMME\", \"SAMME.R\"]),\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = AdaBoostClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def bag_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\n",
    "                \"bootstrap_features\", [True, False]\n",
    "            ),\n",
    "            \"max_samples\": trial.suggest_uniform(\"max_samples\", 0.1, 1),\n",
    "            \"max_features\": trial.suggest_uniform(\"max_features\", 0.1, 1),\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "        clf = BaggingClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def extc_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"min_weight_fraction_leaf\": trial.suggest_float(\n",
    "                \"min_weight_fraction_leaf\", 0, 0.5\n",
    "            ),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 100),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 100),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.01, 1),\n",
    "            \"random_state\": self.random_state,\n",
    "            \"n_jobs\": -1,\n",
    "            \"max_steps\": 10,\n",
    "        }\n",
    "        clf = ExtraTreesClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def lgbm_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"learning_rate\": trial.suggest_categorical(\n",
    "                \"learning_rate\", [0.0125, 0.025, 0.05, 0.1]\n",
    "            ),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2048),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "            \"feature_fraction\": min(\n",
    "                trial.suggest_float(\"feature_fraction\", 0.3, 1.0 + 1e-8), 1.0\n",
    "            ),\n",
    "            \"bagging_fraction\": min(\n",
    "                trial.suggest_float(\"bagging_fraction\", 0.3, 1.0 + 1e-8), 1.0\n",
    "            ),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "            \"extra_trees\": trial.suggest_categorical(\"extra_trees\", [True, False]),\n",
    "            \"feature_pre_filter\": False,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"seed\": self.random_state,\n",
    "            \"num_threads\": -1,\n",
    "            \"objective\": self.lgbm_objective,\n",
    "        }\n",
    "        clf = LGBMClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def cat_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "            \"iterations\": 1000,\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "            \"bootstrap_type\": trial.suggest_categorical(\n",
    "                \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "            ),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "            \"learning_rate\": trial.suggest_categorical(\n",
    "                    \"learning_rate\", [0.05, 0.1, 0.2]\n",
    "                ),\n",
    "            \"rsm\": trial.suggest_float(\"rsm\", 0.1, 1),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\n",
    "                \"l2_leaf_reg\", 0.0001, 10.0, log=False\n",
    "            ),\n",
    "            \"random_state\": self.random_state,\n",
    "            \"verbose\": False,\n",
    "            \"allow_writing_files\": False,\n",
    "        }\n",
    "\n",
    "        if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "            param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "        elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "            param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "        clf = CatBoostClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def xgb_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"learning_rate\": trial.suggest_categorical(\n",
    "                \"learning_rate\", [0.05, 0.1, 0.2]\n",
    "            ),\n",
    "            \"eta\": trial.suggest_categorical(\"eta\", [0.0125, 0.025, 0.05, 0.1]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 10.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "            \"colsample_bytree\": min(\n",
    "                trial.suggest_float(\"colsample_bytree\", 0.3, 1.0 + 1e-8), 1.0\n",
    "            ),\n",
    "            \"subsample\": min(trial.suggest_float(\"subsample\", 0.3, 1.0 + 1e-8), 1.0),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 100),\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"seed\": self.random_state,\n",
    "            \"verbosity\": 0,\n",
    "            \n",
    "        }\n",
    "        clf = XGBClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()\n",
    "\n",
    "    def mlp_classifier_objective(self, trial):\n",
    "        param = {\n",
    "            \"hidden_layer_sizes\": trial.suggest_int(\"hidden_layer_sizes\", 1, 10),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 1, 2000),\n",
    "            \"dense_1_size\": trial.suggest_int(\"dense_1_size\", 4, 100),\n",
    "            \"dense_2_size\": trial.suggest_int(\"dense_2_size\", 2, 100),\n",
    "            \"learning_rate\": trial.suggest_categorical(\n",
    "                \"learning_rate\", [0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "            ),\n",
    "            \"learning_rate_type\": trial.suggest_categorical(\n",
    "                \"learning_rate_type\", [\"constant\", \"adaptive\"]\n",
    "            ),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "            \"seed\": self.random_state,\n",
    "        }\n",
    "        clf = MLPClassifier(**param)\n",
    "        scores = cross_val_score(\n",
    "            clf,\n",
    "            self.X,\n",
    "            self.y,\n",
    "            cv=self.cv,\n",
    "            scoring=self.metric,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56353483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
