{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a370b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Ericd\\OneDrive\\Desktop\\wudimoxing\")\n",
    "from wudi_best import *\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fed49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcesser:\n",
    "    def data_preprocess(\n",
    "        self, features, labels, test_size, random_state, smote, k_neighbors\n",
    "    ):\n",
    "        try:\n",
    "            if smote == \"y\":\n",
    "                sm = SMOTE(k_neighbors=k_neighbors, random_state=random_state)\n",
    "                features, labels = sm.fit_resample(features, labels)\n",
    "            # Splitting ---------------------------------------------------------------------\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                features, labels, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "            # Scaling ---------------------------------------------------------------------\n",
    "            sc = StandardScaler()\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "            X_val = sc.transform(X_val)\n",
    "            return (X_train, X_val, y_train, y_val, sc)\n",
    "        except Exception as error:\n",
    "            print(Fore.RED + \"Preprocessing Failed with error: \", error, \"\\n\")\n",
    "\n",
    "    def confusion_matrix(self, y_pred, y_val):\n",
    "        \"\"\"\n",
    "        Takes Predicted data and Validation data as input and prepares and plots Confusion Matrix.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "            ax = plt.subplot()\n",
    "            sns.heatmap(cm, annot=True, fmt=\"g\", ax=ax)\n",
    "            ax.set_xlabel(\"Predicted labels\")\n",
    "            ax.set_ylabel(\"True labels\")\n",
    "            ax.set_title(\"Confusion Matrix\")\n",
    "            ax.xaxis.set_ticklabels(np.unique(y_val))\n",
    "            ax.yaxis.set_ticklabels(np.unique(y_val))\n",
    "            plt.show()\n",
    "        except Exception as error:\n",
    "            print(\n",
    "                Fore.RED + \"Building Confusion Matrix Failed with error :\", error, \"\\n\"\n",
    "            )\n",
    "\n",
    "    def dimensionality_reduction(\n",
    "        self,\n",
    "        lda,\n",
    "        pca,\n",
    "        X_train,\n",
    "        X_val,\n",
    "        y_train,\n",
    "        n_components_lda,\n",
    "        n_components_pca,\n",
    "        pca_kernel,\n",
    "        start,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs Dimensionality Reduction on Training and Validation independent variables.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if lda == \"y\":\n",
    "                lda = LDA(n_components=n_components_lda)\n",
    "                X_train = lda.fit_transform(X_train, y_train)\n",
    "                X_val = lda.transform(X_val)\n",
    "            if pca == \"y\" and not lda == \"y\":\n",
    "                if not pca_kernel == \"linear\":\n",
    "                    try:\n",
    "\n",
    "                        kpca = KernelPCA(\n",
    "                            n_components=n_components_pca, kernel=pca_kernel\n",
    "                        )\n",
    "                        X_train = kpca.fit_transform(X_train)\n",
    "                        X_val = kpca.transform(X_val)\n",
    "                    except MemoryError as error:\n",
    "                        print(error)\n",
    "                        end = time.time()\n",
    "                        print(\"Time Elapsed :\", end - start)\n",
    "                        return\n",
    "\n",
    "                elif pca_kernel == \"linear\":\n",
    "                    pca = PCA(n_components=n_components_pca)\n",
    "                    X_train = pca.fit_transform(X_train)\n",
    "                    X_val = pca.transform(X_val)\n",
    "                else:\n",
    "                    print(\"Un-identified PCA Kernel\\n\")\n",
    "                    return\n",
    "            return (X_train, X_val)\n",
    "        except Exception as error:\n",
    "            print(\n",
    "                Fore.RED + \"Dimensionality Reduction Failed with error :\", error, \"\\n\"\n",
    "            )\n",
    "            return (X_train, X_val)\n",
    "\n",
    "    def encoder(self, features, labels):\n",
    "        \"\"\"\n",
    "        Takes features and labels as arguments and encodes features using onehot encoding and labels with label encoding.\n",
    "        Returns Encoded Features and Labels.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cat_features = [\n",
    "                i for i in features.columns if features.dtypes[i] == \"object\"\n",
    "            ]\n",
    "            if len(cat_features) >= 1:\n",
    "                index = []\n",
    "                for i in range(0, len(cat_features)):\n",
    "                    index.append(features.columns.get_loc(cat_features[i]))\n",
    "                ct = ColumnTransformer(\n",
    "                    transformers=[(\"encoder\", OneHotEncoder(), index)],\n",
    "                    remainder=\"passthrough\",\n",
    "                )\n",
    "                print(\"Encoding Features [*]\\n\")\n",
    "                features = np.array(ct.fit_transform(features))\n",
    "            if labels.dtype == \"O\":\n",
    "                le = LabelEncoder()\n",
    "                labels = le.fit_transform(labels)\n",
    "            return (features, labels)\n",
    "        except Exception as error:\n",
    "            print(Fore.RED + \"Encoding Failed with error :\", error)\n",
    "\n",
    "\n",
    "def permutational_feature_imp(features, X_test, y_test, model):\n",
    "    # Compute permutation importance\n",
    "    perm_importance = permutation_importance(model, X_test, y_test)\n",
    "\n",
    "    # Create a dictionary to group features\n",
    "    feature_groups = {\n",
    "        \"Geological Factors\": [\"City name\", \"City population\"],\n",
    "        \"Chemical Features\": [\"Nitrogen\", \"Phosphorus\", \"Ammonium\"]\n",
    "        # Add more groups and features as needed\n",
    "    }\n",
    "\n",
    "    # Create dictionaries to hold the grouped feature importances\n",
    "    group_importances = {group: [] for group in feature_groups}\n",
    "    feature_importances = {}\n",
    "\n",
    "    for group, group_features in feature_groups.items():\n",
    "        # Get the indices of features belonging to the current group\n",
    "        group_indices = [features.columns.get_loc(feat) for feat in group_features]\n",
    "        # Calculate the mean importance for the features in the current group\n",
    "        group_importance = perm_importance.importances_mean[group_indices].mean()\n",
    "        group_importances[group].append(group_importance)\n",
    "        for feat in group_features:\n",
    "            feature_importances[feat] = group_importance\n",
    "\n",
    "    # Sort the grouped feature importances based on importance in descending order\n",
    "    sorted_group_importances = sorted(group_importances.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "    # Extract the group names and importances from the sorted list\n",
    "    group_names, group_importances = zip(*sorted_group_importances)\n",
    "\n",
    "    # Create a horizontal bar plot to visualize the grouped feature importances\n",
    "    fig, ax = plt.subplots()\n",
    "    for idx, group_name in enumerate(group_names):\n",
    "        ax.barh(group_name, group_importances[idx][0])\n",
    "\n",
    "    for feat in features.columns:\n",
    "        ax.barh(feat, feature_importances[feat])\n",
    "\n",
    "    ax.set_xlabel(\"Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your data and model\n",
    "permutational_feature_imp(features, X_test, y_test, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def shap_feature_imp(self, features, X_train, model, *args, **kwargs):\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_train)\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_train,\n",
    "            feature_names=features.columns,\n",
    "            plot_type=\"bar\",\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        shap.summary_plot(\n",
    "            shap_values, X_train, feature_names=features.columns, *args, **kwargs\n",
    "        )\n",
    "        for i in range(len(features.columns)):\n",
    "            shap.dependence_plot(\n",
    "                i, shap_values, X_train, feature_names=features.columns, *args, **kwargs\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6c7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
